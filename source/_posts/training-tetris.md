title : Tetris 학습시키기
date: 2017-03-07 12:00:00
---


rlcode가 만들어진 목표는 알까기를 학습시키기 위함이었습니다. 막상 알까기를 학습시키려고 하니 어려운 점들이 하나 둘이 아니었습니다.  먼저 알까기가 루비로 돼있어 귀찮은 점이 한두가지가 아니었습니다. 행동공간(action space)이 연속이라 그 당시 저희의 실력으로는 풀기 어려운 문제이기도 했습니다. rlcode의 첫 프로젝트는 그렇게...  나중을 기약하기로 합니다. 다음 프로젝트를 고민하던 중 남들이 다 하는 openai를 이용하지 말고 pygame으로 된 우리만의 게임을 하기로 결정합니다. **테트리스!!**

<!--more-->

## 테트리스 학습시키기

먼저 저희는 자료를 찾았습니다. 가장 먼저 pygame에서 테트리스를 찾았습니다. [Making Games with Python & Pygame](https://inventwithpython.com/makinggames.pdf)에 pygame으로 만든 테트리스 코드가 있어 참조해 만들었습니다. 똑같이.

이제 학습을 시켜야 합니다. 처음에는 아무 생각 없이 20x10 데이터를 가지고 일반적인 Neural Network를 이용해 DQN 에이전트를 만들었습니다. 20x10에서 빈 공간은 0, 블럭이 있는 공간은 1로 만들어서 상태(state)를 정의했죠. 하지만 잘 안됐습니다. 그 당시 몇가지 이유를 생각해 봤습니다.

1. 네트워크가 작은 경우
2. 행동이 너무 많아서 안되는 경우
3. 하이퍼 파라미터가 잘못 정해진 경우
4. 보상을 잘못 준 경우
5. 그냥 코드상의 문제



가장 먼저 생각한 부분은 행동이 너무 많은 경우였습니다. 처음 정의된 행동은 [왼쪽, 오른쪽, 아래, 회전]이었습니다. 행동을 이렇게 정의하면 하나의 보상을 받을 때 까지 너무 많은 행동을 하게됩니다. 보상을 받을 때까지 많은 행동을 하면 어느 행동을 잘해서 또는 어느 행동을 잘못해서 보상을 받았는지 알기 힘듭니다. 그래서 행동을 다시 정의 했습니다. 나타나는 위치(왼쪽에서 얼마나 떨어져 있는가)와 회전된 모양을 행동으로 정의하기로 했습니다. 중간에 자잘한 행동들을 하나의 행동으로 만들었습니다. 그냥 행동만 바꿔서는 잘 학습이 되지 않았습니다.



두번째는 보상을 생각했습니다. 테트리스는 기본적으로 한줄을 없애면 보상을 +1 받도록 돼있습니다. [Playing Tetris with Deep Reinforcement Learning](http://cs231n.stanford.edu/reports2016/121_Report.pdf)을 보면 에이전트가 다른 방식으로 보상을 받도록 합니다. 그래서 저희도 보상을 조금 바꿔보기로 했습니다. 보상을 너무 많이 바꾸는 것은 강화학습의 의도에 맞지 않을 것 같아서 살짝만 바꿨습니다.

![구멍](/images/holes.png)

구멍을 보상에 추가했습니다. 사람이 실제 플레이 할 때 구멍을 최대한 적게 만듭니다. 저희도 구멍이 생기는 것을 적게 하기 위해 구멍개수 만큼 -1 보상을 받도록 했습니다. 그리고 여러 줄을 없앨 때 더 큰 보상을 받게 하기 위해 (없앤 줄 수의 제곱) * 10 이라는 보상을 줬습니다.


세번째는 네트워크 크기를 생각했습니다. 처음에는 단순한 Neural Network를 이용했다가 후에는 CNN(Convolutional Neural Network)을 이용했습니다. 저희는 아래와 같은 네트워크를 이용해 학습을 시켜봤습니다.

컨볼루션 계층 1-16(filter size 4x4, stride 1x1), 컨볼루션 계층16-32(filter size 4x4, stride 2x2), 완전 연결 계층(Fully-connected layer) 1600-512, 완전 연결 계층 512-14 (나타날 수 있는 위치 10, 회전할 수 있는 모양 4)



이런 몇가지 부분들을 고려해서 바꾸고 나서도 학습이 제대로 되지 않았습니다. 안되는 이유를 모르니 저희는 일단 두번째 프로젝트도 접어두기로 합니다. 현재는 책을 쓰고 있죠. 책을 쓰면서 테트리스나 알까기보다 조금 더 쉬운 예제들을 다루면서 강화학습을 더 공부하고 다시 보기로 했습니다. 



## 이미 알려진 상태가치함수를 이용한 트리탐색

저희는 강화학습으로 테트리스를 하는 것에 한계를 느꼈지만 인공지능 에이전트가 테트리스를 플레이하는 모습을 보고싶었습니다. 그래서 이미 알려진 상태가치함수를 이용해서 에이전트가 트리탐색을 통해 행동을 하도록 만들었습니다. [Tetris AI](https://codemyroad.wordpress.com/2013/04/14/tetris-ai-the-near-perfect-player/)에서는 유전 알고리즘을 통해 가치함수를 찾는 방법이 소개되어 있습니다. 여기에서 알려준 가치함수를 이용해서 각각의 행동을 해보고 가장 높은 가치을 받을 행동을 하도록 에이전트를 만들었습니다.

[![플레이영상](https://img.youtube.com/vi/QZys5ZURqnc/0.jpg)](https://www.youtube.com/watch?v=QZys5ZURqnc?t=0s)

## 테트리스 학습을 통해 느낀 한계

아직까지 테트리스를 포기한 상태는 아니지만 테트리스 학습을 통해 몇가지 강화학습의 한계를 느꼈습니다. 저희가 체계적으로 학습을 시키고 여러가지 디버깅을 해보지 못한 점도 문제였습니다. 테트리스 실패를 통해 강화학습이 생각보다 더 어렵다는 점을 알았고 문제를 풀 때 더 체계적인 접근이 필요하다는 점도 깨달았습니다.

결과적으로 여러 부분들을 다듬고 보완해야 한다는 사실을 깨닫게 되는 프로젝트였습니다. 결과는 좋지 않았지만 저희 팀에게는 많은 생각을 할 수 있는 프로젝트였습니다.

